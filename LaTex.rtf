{\rtf1\ansi\ansicpg1252\cocoartf2820
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \\documentclass[12pt]\{article\}\
\\usepackage\{amsmath\}     % For advanced math formatting\
\\usepackage\{graphicx\}    % For including images\
\\usepackage\{ragged2e\}    % For flexible text alignment\
\\usepackage\{times\}       % Times New Roman font\
\\usepackage\{geometry\}    % For setting the font size\
\\usepackage\{setspace\}    % For double spacing\
\\usepackage\{listings\}    % To format PYTHON code blocks\
\\usepackage\{xcolor\}      % For using colors\
\
\\geometry\{a4paper, left=1in, top=1in, right=1in, bottom=1in\}\
\\doublespacing  % Apply double line spacing\
\
% Define custom fields for instructor, class, and number\
\\newcommand\{\\instructor\}\{Dr. Tan\}\
\\newcommand\{\\classTitle\}\{Applied Machine Learning in Economics\}\
\\newcommand\{\\classNumber\}\{ECON-6930-01\}\
\
% Define colors for text\
\\lstset\{\
    language=Python,\
    commentstyle=\\color\{gray\},\
    basicstyle=\\ttfamily,\
    keywordstyle=\\color\{blue\},\
    stringstyle=\\color\{red\},\
    emphstyle=\\color\{green!60!black\},\
    breaklines=true\
\}\
\
\\begin\{document\}\
\
\\centering\
\\includegraphics[width=0.75\\linewidth]\{1.png\} \\\\\
\
\\textbf\{Title:\} PS2  \\\\\
\\textbf\{Author:\} Vitor Hugo Silva dos Santos \\\\    \
\\textbf\{Instructor:\} \\instructor \\\\\
\\textbf\{Class:\} \\classTitle \\ (\\classNumber) \\\\\
\\textbf\{Date:\} \\today\
\
\\pagebreak\
\\RaggedRight\
\
\\section\{Introduction\}\
In this assignment, I worked on implementing linear classifiers using hinge and softmax loss functions. The goal was to test how well these classifiers work on a spiral dataset. I also used regularization to prevent overfitting and applied gradient descent to improve the model over time.\
\
\\section\{Methodology\}\
\
\\subsection\{Score Function\}\
The score function is a simple calculation. It multiplies the input features by the weights and adds a bias. This can be written like this:\
\
\\[\
\\text\{scores\} = XW + b\
\\]\
\
Where \\( X \\) is the data, \\( W \\) is the weights, and \\( b \\) is the bias. This is the basic step used before calculating losses.\
\
\\subsection\{Hinge Loss\}\
Hinge loss is mostly used in SVMs. It calculates how far off the prediction is from the correct label, but only when it's really off. The formula is:\
\
\\[\
L_\{\\text\{hinge\}\} = \\max(0, 1 - y \\cdot \\text\{score\})\
\\]\
\
I also used L2 regularization to keep the weights from getting too big:\
\
\\[\
\\text\{L2 Regularization\} = \\lambda \\sum W^2\
\\]\
\
\\subsection\{Softmax Loss\}\
Softmax loss is used when there are multiple classes. It calculates the probability of each class and measures how far off it is from the true label:\
\
\\[\
L_\{\\text\{softmax\}\} = -\\log\\left( \\frac\{\\exp(\\text\{score\}_\{y_i\})\}\{\\sum_j \\exp(\\text\{score\}_j)\} \\right)\
\\]\
\
I used L2 regularization here too, to keep the model from overfitting.\
\
\\subsection\{Gradient Descent\}\
I used gradient descent to make the model better by adjusting the weights and bias. In each step, it calculates how much the loss changes for small changes in \\( W \\) and \\( b \\), and then updates them:\
\
\\[\
W = W - \\eta \\cdot \\nabla_W L\
\\]\
\\[\
b = b - \\eta \\cdot \\nabla_b L\
\\]\
\
Where \\( \\eta \\) is the learning rate, which controls how big each update is.\
\
\\section\{Results\}\
\
I tested the model on a spiral dataset. Here are the results from the loss functions, both with and without regularization.\
\
\\subsection\{Task 1: Score Calculation\}\
The calculated scores were:\
\
\\[\
\\text\{Scores\} =\
\\begin\{bmatrix\}\
0.66 & 0.94 \\\\\
0.6  & 0.7  \\\\\
\\end\{bmatrix\}\
\\]\
\
\\subsection\{Task 2: Loss without Regularization\}\
\\begin\{itemize\}\
    \\item \\textbf\{Hinge Loss\}: 0.8067\
    \\item \\textbf\{Softmax Loss\}: 1.0331\
\\end\{itemize\}\
\
\\subsection\{Task 3: Loss with L2 Regularization\}\
\\begin\{itemize\}\
    \\item \\textbf\{Hinge Loss (with L2 regularization)\}: 0.8327\
    \\item \\textbf\{Softmax Loss (with L2 regularization)\}: 1.1291\
\\end\{itemize\}\
\
\\subsection\{Task 4: Gradient Descent Optimization\}\
\
For the hinge loss function:\
\
\\[\
\\text\{Optimized Hinge Weights\} = [0.506, 0.601], \\, \\text\{Optimized Bias\} = 0.42\
\\]\
\
For the softmax loss function:\
\
\\[\
\\text\{Optimized Softmax Weights\} = \
\\begin\{bmatrix\}\
0.244 & 0.601 \\\\\
0.088 & 0.735 \\\\\
0.651 & 0.137\
\\end\{bmatrix\}, \\, \\text\{Optimized Bias\} = [0.129, 0.205, 0.266]\
\\]\
\
\\section\{Conclusion\}\
I learned a lot about hinge and softmax loss functions and how gradient descent helps improve the model. Regularization made a big difference in keeping the model from overfitting. The hardest part was finding the right balance with the learning rate and regularization. This assignment helped me understand these concepts better, and it was great to apply them to a real dataset.\
\
\\end\{document\}}